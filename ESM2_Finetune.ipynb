{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzwTvNw8qn0v"
   },
   "source": [
    "Connect GPU and istall fair-esm package for ESM2 usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oQKLvFxRlw-c",
    "outputId": "74417bbe-9df1-42f1-b1e4-31056431f91e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "!pip install fair-esm -q\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. read in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mutated_sequence(mut, sequence_wt):\n",
    "  wt, pos, mt = mut[0], int(mut[1:-1]), mut[-1]\n",
    "\n",
    "  sequence = deepcopy(sequence_wt)\n",
    "\n",
    "  return sequence[:pos]+mt+sequence[pos+1:]\n",
    "\n",
    "def get_pos_sequence(mut):\n",
    "  wt, pos, mt = mut[0], int(mut[1:-1]), mut[-1]\n",
    "\n",
    "  return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Hackathon_data/sequence.fasta', 'r') as f:\n",
    "  data = f.readlines()\n",
    "\n",
    "sequence_wt = data[1].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "df_base = pd.read_csv('Hackathon_data/train.csv')\n",
    "df_base['mut_seq'] = df_base.mutant.apply(lambda x: get_mutated_sequence(x, sequence_wt))\n",
    "df_base['pos'] = df_base.mutant.apply(lambda x: get_pos_sequence(x))\n",
    "df_base['wt_seq'] = sequence_wt\n",
    "df_base.loc[:, \"score\"] = df_base[\"DMS_score\"]\n",
    "\n",
    "df_train2 = pd.read_csv('Hackathon_data/train2.csv')\n",
    "df_train2['mut_seq'] = df_train2.mutant.apply(lambda x: get_mutated_sequence(x, sequence_wt))\n",
    "df_train2['pos'] = df_train2.mutant.apply(lambda x: get_pos_sequence(x))\n",
    "df_train2['wt_seq'] = sequence_wt\n",
    "df_train2.loc[:, \"score\"] = df_train2[\"DMS_score\"]\n",
    "\n",
    "df_train3 = pd.read_csv('Hackathon_data/train3.csv')\n",
    "df_train3['mut_seq'] = df_train3.mutant.apply(lambda x: get_mutated_sequence(x, sequence_wt))\n",
    "df_train3['pos'] = df_train3.mutant.apply(lambda x: get_pos_sequence(x))\n",
    "df_train3['wt_seq'] = sequence_wt\n",
    "df_train3.loc[:, \"score\"] = df_train3[\"DMS_score\"]\n",
    "\n",
    "df_train4 = pd.read_csv('Hackathon_data/train4.csv')\n",
    "df_train4['mut_seq'] = df_train4.mutant.apply(lambda x: get_mutated_sequence(x, sequence_wt))\n",
    "df_train4['pos'] = df_train4.mutant.apply(lambda x: get_pos_sequence(x))\n",
    "df_train4['wt_seq'] = sequence_wt\n",
    "df_train4.loc[:, \"score\"] = df_train4[\"DMS_score\"]\n",
    "\n",
    "df_test = pd.read_csv('Hackathon_data/test.csv')\n",
    "df_test['mut_seq'] = df_test.mutant.apply(lambda x: get_mutated_sequence(x, sequence_wt))\n",
    "df_test['pos'] = df_test.mutant.apply(lambda x: get_pos_sequence(x))\n",
    "df_test['wt_seq'] = sequence_wt\n",
    "df_test.loc[:, \"score\"] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate initial training data and following query data\n",
    "df_train = pd.concat([df_base, df_train2, df_train3, df_train4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTXj9A5Sq0ZK"
   },
   "source": [
    "# 2 Define Dataset and Model\n",
    "this part was referred from https://github.com/moritzgls/ESM-Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Cj2rXjmllw-e"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "import matplotlib.pyplot as plt\n",
    "import esm\n",
    "\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "\n",
    "        self.df = df\n",
    "        self._setup_esm()\n",
    "\n",
    "    def _setup_esm(self):\n",
    "\n",
    "        _, self.esm_alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "        self.esm_batch_converter = self.esm_alphabet.get_batch_converter()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        \n",
    "        # Base features\n",
    "        _, _, esm_batch_tokens1 = self.esm_batch_converter(\n",
    "            [('', ''.join(row['wt_seq'])[:1022])])\n",
    "        _, _, esm_batch_tokens2 = self.esm_batch_converter(\n",
    "            [('', ''.join(row['mut_seq'])[:1022])])\n",
    "        \n",
    "        tokens1 = esm_batch_tokens1[0]\n",
    "        tokens2 = esm_batch_tokens2[0]\n",
    "        \n",
    "        pos = torch.tensor(row['pos'], dtype=torch.long)\n",
    "        length = torch.tensor(len(row['wt_seq']), dtype=torch.long)\n",
    "    \n",
    "\n",
    "        # position and length features\n",
    "        features = [\n",
    "            tokens1,\n",
    "            tokens2,\n",
    "            row['pos'],\n",
    "            len(row[\"wt_seq\"]),\n",
    "        ]\n",
    "\n",
    "        # Target\n",
    "        features.append(\n",
    "            torch.unsqueeze(torch.FloatTensor([row['score']]), 0)\n",
    "        )\n",
    "\n",
    "        return tuple(features)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPocg077xeYQ"
   },
   "source": [
    "Define an ESM-finetuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "class ESMEffect(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.freeze_up_to = 31\n",
    "\n",
    "        self.esm2mut, _ = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "\n",
    "        # Freeze layers up to the 31th layer\n",
    "        for i in range(self.freeze_up_to):\n",
    "            for param in self.esm2mut.layers[i].parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.n_layers = 33\n",
    "        embedding_dim = 1280\n",
    "\n",
    "        # Cache for embeddings from the first 31 layers\n",
    "        self.embedding_cache = defaultdict(torch.Tensor)\n",
    "\n",
    "        # Regression head\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.regressor = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, token_wt, token_mut, pos, lengths):\n",
    "        batch_size = token_mut.size(0)\n",
    "        cached_mut_embeddings = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            seq_mut_id = token_mut[i].tolist() \n",
    "            seq_wt_id = token_wt[i].tolist()\n",
    "            if tuple(seq_mut_id) in self.embedding_cache:\n",
    "                # Use cached embedding\n",
    "                cached_mut_embeddings.append(self.embedding_cache[tuple(seq_mut_id)])\n",
    "            else:\n",
    "                # Generate new mutant embedding and cache it\n",
    "                with torch.no_grad():\n",
    "                    x = self.esm2mut(token_mut[i].unsqueeze(0), [self.n_layers-2])\n",
    "                embedding = x['representations'][self.n_layers-2]\n",
    "                self.embedding_cache[tuple(seq_mut_id)] = embedding.detach() \n",
    "                cached_mut_embeddings.append(embedding)\n",
    "            \n",
    "\n",
    "        # Stack cached embeddings for the batch\n",
    "        mut = torch.cat(cached_mut_embeddings, dim=0)\n",
    "\n",
    "        # Pass mutant embedding through 32th and 33th layers\n",
    "        mut, _ = self.esm2mut.layers[self.n_layers-2](mut)\n",
    "        mut, _ = self.esm2mut.layers[self.n_layers-1](mut)\n",
    "        mut = self.esm2mut.emb_layer_norm_after(mut)\n",
    "        \n",
    "        # Extract the embedding corresponding to the mutated residue\n",
    "        x = []\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            position =  mut[i, pos[i]+1, :]\n",
    "            x.append(position)\n",
    "\n",
    "        x = torch.stack(x).squeeze(1)\n",
    "\n",
    "        # Feed through the regression head\n",
    "        predictions = self.regressor(self.dropout(x))\n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "def setup_optimizer_and_scheduler_esmeffect(model, train_loader, epochs, batch_size, lr_for_esm, lr_for_head):\n",
    "\n",
    "    # Define learning rates\n",
    "    lr_esm =  batch_size * lr_for_esm  # Lower learning rate for pre-trained ESM2 layers\n",
    "    lr_new =  batch_size * lr_for_head  # Higher learning rate for new layers (regression head)\n",
    "\n",
    "    # Group parameters\n",
    "    esm_params = list(model.esm2mut.parameters())\n",
    "    new_params = (\n",
    "        list(model.regressor.parameters())\n",
    "    )\n",
    "\n",
    "    # Create parameter groups with different learning rates\n",
    "    param_groups = [\n",
    "        {'params': esm_params, 'lr': lr_esm},\n",
    "        {'params': new_params, 'lr': lr_new}\n",
    "    ]\n",
    "\n",
    "    # Initialize the optimizer with parameter groups\n",
    "    optimizer = AdamW(param_groups, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "\n",
    "    # Create the scheduler\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=[lr_esm, lr_new],  # Specify max_lr for each group\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.stats import spearmanr\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        batch = [b.to(device) if isinstance(b, torch.Tensor) else b for b in batch]\n",
    "        inputs, labels = batch[:-1], batch[-1].float()\n",
    "        \n",
    "        with autocast('cuda'):\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(*inputs)\n",
    "            loss = pairwise_ranking_loss(predictions, labels.squeeze(2))\n",
    "        \n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update() \n",
    "    \n",
    "            scheduler.step()\n",
    "            \n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = [b.to(device) if isinstance(b, torch.Tensor) else b for b in batch]\n",
    "            inputs, labels = batch[:-1], batch[-1].float()\n",
    "            \n",
    "            with autocast('cuda'):\n",
    "                predictions = model(*inputs)\n",
    "                loss = pairwise_ranking_loss(predictions, labels.squeeze(2))\n",
    "                \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    all_preds = np.squeeze(all_preds)\n",
    "    all_labels = np.squeeze(all_labels)\n",
    "            \n",
    "            \n",
    "    metrics = {\n",
    "        f\"loss\": total_loss / len(loader),\n",
    "        f\"spearman\": spearmanr(all_labels, all_preds)[0],\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def predict(model, dataframe, config,device):\n",
    "    model.eval()\n",
    "    dataset = ProteinDataset(dataframe)\n",
    "    loader = DataLoader(dataset, batch_size=8, num_workers=config['num_workers'],shuffle=False)\n",
    "    \n",
    "    \n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = [b.to(device) if isinstance(b, torch.Tensor) else b for b in batch]\n",
    "            preds = model(*batch[:-1])  # Exclude labels\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            \n",
    "    predicts = np.squeeze(all_preds)\n",
    "    \n",
    "    return predicts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a Training Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'batch_size': 32,\n",
    "    'epochs': 30,\n",
    "    'criterion': 'MSELoss',\n",
    "    'lr_esm': 1e-5,\n",
    "    'lr_head': 1e-4,\n",
    "    'num_workers': 4,\n",
    "    'feature_columns': []  # Add or remove features as needed (experimental feature, column must be present in dataset)\n",
    "}\n",
    "\n",
    "# use pairwise ranking as training loss\n",
    "def pairwise_ranking_loss(preds, targets, margin=1.0):\n",
    "    \"\"\"\n",
    "    preds: Tensor of shape (B, 1) – predicted DMS scores\n",
    "    targets: Tensor of shape (B, 1) – true DMS scores\n",
    "    \"\"\"\n",
    "    preds = preds.squeeze() \n",
    "    targets = targets.squeeze()\n",
    "    diff_target = targets.unsqueeze(0) - targets.unsqueeze(1)\n",
    "    diff_pred = preds.unsqueeze(0) - preds.unsqueeze(1)\n",
    "    target_sign = torch.sign(diff_target)\n",
    "    loss_matrix = F.relu(-target_sign * diff_pred + margin)\n",
    "    mask = torch.eye(len(targets), dtype=torch.bool, device=targets.device)\n",
    "    loss_matrix = loss_matrix[~mask].view(len(targets), -1)\n",
    "    return loss_matrix.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1/5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/yshen476/.conda/envs/torch/lib/python3.9/site-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/home/hice1/yshen476/.conda/envs/torch/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train Loss: 0.9779 | Val loss:0.9956 | Val Spearman: 0.3413\n",
      "New best Spearman: 0.3413 — Model saved.\n",
      "[Epoch 1] Train Loss: 0.9192 | Val loss:0.9844 | Val Spearman: 0.3741\n",
      "New best Spearman: 0.3741 — Model saved.\n",
      "[Epoch 2] Train Loss: 0.8132 | Val loss:0.9618 | Val Spearman: 0.3993\n",
      "New best Spearman: 0.3993 — Model saved.\n",
      "[Epoch 3] Train Loss: 0.6913 | Val loss:0.9182 | Val Spearman: 0.4169\n",
      "New best Spearman: 0.4169 — Model saved.\n",
      "[Epoch 4] Train Loss: 0.6404 | Val loss:0.8963 | Val Spearman: 0.4437\n",
      "New best Spearman: 0.4437 — Model saved.\n",
      "[Epoch 5] Train Loss: 0.6302 | Val loss:0.8871 | Val Spearman: 0.4444\n",
      "New best Spearman: 0.4444 — Model saved.\n",
      "[Epoch 6] Train Loss: 0.6346 | Val loss:0.8680 | Val Spearman: 0.4259\n",
      "[Epoch 7] Train Loss: 0.6179 | Val loss:0.8706 | Val Spearman: 0.4285\n",
      "[Epoch 8] Train Loss: 0.6027 | Val loss:0.8629 | Val Spearman: 0.4473\n",
      "New best Spearman: 0.4473 — Model saved.\n",
      "[Epoch 9] Train Loss: 0.6024 | Val loss:0.8464 | Val Spearman: 0.4533\n",
      "New best Spearman: 0.4533 — Model saved.\n",
      "[Epoch 10] Train Loss: 0.5929 | Val loss:0.8474 | Val Spearman: 0.4646\n",
      "New best Spearman: 0.4646 — Model saved.\n",
      "[Epoch 11] Train Loss: 0.5802 | Val loss:0.8432 | Val Spearman: 0.4780\n",
      "New best Spearman: 0.4780 — Model saved.\n",
      "[Epoch 12] Train Loss: 0.5773 | Val loss:0.8361 | Val Spearman: 0.4648\n",
      "[Epoch 13] Train Loss: 0.5783 | Val loss:0.8460 | Val Spearman: 0.4187\n",
      "[Epoch 14] Train Loss: 0.5587 | Val loss:0.8319 | Val Spearman: 0.4352\n",
      "[Epoch 15] Train Loss: 0.5639 | Val loss:0.8284 | Val Spearman: 0.4497\n",
      "[Epoch 16] Train Loss: 0.5476 | Val loss:0.8210 | Val Spearman: 0.4559\n",
      "Early stopping triggered after 17 epochs.\n",
      "Fold 1 Best_Spearman: 0.4780\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "[Epoch 0] Train Loss: 0.9754 | Val loss:0.9897 | Val Spearman: 0.0180\n",
      "New best Spearman: 0.0180 — Model saved.\n",
      "[Epoch 1] Train Loss: 0.9065 | Val loss:0.9691 | Val Spearman: 0.3201\n",
      "New best Spearman: 0.3201 — Model saved.\n",
      "[Epoch 2] Train Loss: 0.7949 | Val loss:0.9256 | Val Spearman: 0.4515\n",
      "New best Spearman: 0.4515 — Model saved.\n",
      "[Epoch 3] Train Loss: 0.6937 | Val loss:0.9092 | Val Spearman: 0.4970\n",
      "New best Spearman: 0.4970 — Model saved.\n",
      "[Epoch 4] Train Loss: 0.6888 | Val loss:0.8843 | Val Spearman: 0.5438\n",
      "New best Spearman: 0.5438 — Model saved.\n",
      "[Epoch 5] Train Loss: 0.6144 | Val loss:0.8814 | Val Spearman: 0.5366\n",
      "[Epoch 6] Train Loss: 0.6136 | Val loss:0.8484 | Val Spearman: 0.5663\n",
      "New best Spearman: 0.5663 — Model saved.\n",
      "[Epoch 7] Train Loss: 0.6202 | Val loss:0.8166 | Val Spearman: 0.5857\n",
      "New best Spearman: 0.5857 — Model saved.\n",
      "[Epoch 8] Train Loss: 0.5984 | Val loss:0.8378 | Val Spearman: 0.5800\n",
      "[Epoch 9] Train Loss: 0.5779 | Val loss:0.8065 | Val Spearman: 0.5844\n",
      "[Epoch 10] Train Loss: 0.5776 | Val loss:0.7973 | Val Spearman: 0.6100\n",
      "New best Spearman: 0.6100 — Model saved.\n",
      "[Epoch 11] Train Loss: 0.6511 | Val loss:0.8154 | Val Spearman: 0.6057\n",
      "[Epoch 12] Train Loss: 0.5367 | Val loss:0.7902 | Val Spearman: 0.5925\n",
      "[Epoch 13] Train Loss: 0.5636 | Val loss:0.8586 | Val Spearman: 0.5430\n",
      "[Epoch 14] Train Loss: 0.6206 | Val loss:0.8369 | Val Spearman: 0.5689\n",
      "[Epoch 15] Train Loss: 0.5671 | Val loss:0.7926 | Val Spearman: 0.5768\n",
      "Early stopping triggered after 16 epochs.\n",
      "Fold 2 Best_Spearman: 0.6100\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "[Epoch 0] Train Loss: 0.9794 | Val loss:0.9885 | Val Spearman: 0.1529\n",
      "New best Spearman: 0.1529 — Model saved.\n",
      "[Epoch 1] Train Loss: 0.9121 | Val loss:0.9625 | Val Spearman: 0.2467\n",
      "New best Spearman: 0.2467 — Model saved.\n",
      "[Epoch 2] Train Loss: 0.7821 | Val loss:0.9257 | Val Spearman: 0.3152\n",
      "New best Spearman: 0.3152 — Model saved.\n",
      "[Epoch 3] Train Loss: 0.6625 | Val loss:0.9079 | Val Spearman: 0.3756\n",
      "New best Spearman: 0.3756 — Model saved.\n",
      "[Epoch 4] Train Loss: 0.6358 | Val loss:0.8951 | Val Spearman: 0.4219\n",
      "New best Spearman: 0.4219 — Model saved.\n",
      "[Epoch 5] Train Loss: 0.6200 | Val loss:0.8978 | Val Spearman: 0.4302\n",
      "New best Spearman: 0.4302 — Model saved.\n",
      "[Epoch 6] Train Loss: 0.6078 | Val loss:0.8913 | Val Spearman: 0.4452\n",
      "New best Spearman: 0.4452 — Model saved.\n",
      "[Epoch 7] Train Loss: 0.5930 | Val loss:0.8831 | Val Spearman: 0.4545\n",
      "New best Spearman: 0.4545 — Model saved.\n",
      "[Epoch 8] Train Loss: 0.5956 | Val loss:0.8401 | Val Spearman: 0.5369\n",
      "New best Spearman: 0.5369 — Model saved.\n",
      "[Epoch 9] Train Loss: 0.5734 | Val loss:0.8475 | Val Spearman: 0.5319\n",
      "[Epoch 10] Train Loss: 0.5689 | Val loss:0.8482 | Val Spearman: 0.4948\n",
      "[Epoch 11] Train Loss: 0.5719 | Val loss:0.8370 | Val Spearman: 0.5443\n",
      "New best Spearman: 0.5443 — Model saved.\n",
      "[Epoch 12] Train Loss: 0.5574 | Val loss:0.8448 | Val Spearman: 0.5359\n",
      "[Epoch 13] Train Loss: 0.5388 | Val loss:0.8466 | Val Spearman: 0.5375\n",
      "[Epoch 14] Train Loss: 0.5407 | Val loss:0.8542 | Val Spearman: 0.5412\n",
      "[Epoch 15] Train Loss: 0.5012 | Val loss:0.8401 | Val Spearman: 0.5495\n",
      "New best Spearman: 0.5495 — Model saved.\n",
      "[Epoch 16] Train Loss: 0.5005 | Val loss:0.8498 | Val Spearman: 0.5345\n",
      "[Epoch 17] Train Loss: 0.4823 | Val loss:0.8401 | Val Spearman: 0.5463\n",
      "[Epoch 18] Train Loss: 0.4733 | Val loss:0.8303 | Val Spearman: 0.5600\n",
      "New best Spearman: 0.5600 — Model saved.\n",
      "[Epoch 19] Train Loss: 0.4482 | Val loss:0.8472 | Val Spearman: 0.5483\n",
      "[Epoch 20] Train Loss: 0.4524 | Val loss:0.8450 | Val Spearman: 0.5457\n",
      "[Epoch 21] Train Loss: 0.4274 | Val loss:0.8545 | Val Spearman: 0.5381\n",
      "[Epoch 22] Train Loss: 0.4297 | Val loss:0.8512 | Val Spearman: 0.5429\n",
      "[Epoch 23] Train Loss: 0.4106 | Val loss:0.8652 | Val Spearman: 0.5259\n",
      "Early stopping triggered after 24 epochs.\n",
      "Fold 3 Best_Spearman: 0.5600\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "[Epoch 0] Train Loss: 0.9771 | Val loss:0.9898 | Val Spearman: -0.0573\n",
      "New best Spearman: -0.0573 — Model saved.\n",
      "[Epoch 1] Train Loss: 0.9123 | Val loss:0.9709 | Val Spearman: 0.2465\n",
      "New best Spearman: 0.2465 — Model saved.\n",
      "[Epoch 2] Train Loss: 0.7903 | Val loss:0.9286 | Val Spearman: 0.3936\n",
      "New best Spearman: 0.3936 — Model saved.\n",
      "[Epoch 3] Train Loss: 0.6685 | Val loss:0.9155 | Val Spearman: 0.4669\n",
      "New best Spearman: 0.4669 — Model saved.\n",
      "[Epoch 4] Train Loss: 0.6205 | Val loss:0.8943 | Val Spearman: 0.4828\n",
      "New best Spearman: 0.4828 — Model saved.\n",
      "[Epoch 5] Train Loss: 0.6200 | Val loss:0.8995 | Val Spearman: 0.4827\n",
      "[Epoch 6] Train Loss: 0.6075 | Val loss:0.8917 | Val Spearman: 0.4785\n",
      "[Epoch 7] Train Loss: 0.6043 | Val loss:0.8868 | Val Spearman: 0.5146\n",
      "New best Spearman: 0.5146 — Model saved.\n",
      "[Epoch 8] Train Loss: 0.5903 | Val loss:0.8778 | Val Spearman: 0.4849\n",
      "[Epoch 9] Train Loss: 0.5882 | Val loss:0.8764 | Val Spearman: 0.5196\n",
      "New best Spearman: 0.5196 — Model saved.\n",
      "[Epoch 10] Train Loss: 0.5746 | Val loss:0.8615 | Val Spearman: 0.5142\n",
      "[Epoch 11] Train Loss: 0.5537 | Val loss:0.8507 | Val Spearman: 0.5145\n",
      "[Epoch 12] Train Loss: 0.5504 | Val loss:0.8635 | Val Spearman: 0.4880\n",
      "[Epoch 13] Train Loss: 0.5442 | Val loss:0.8526 | Val Spearman: 0.5092\n",
      "[Epoch 14] Train Loss: 0.5245 | Val loss:0.8574 | Val Spearman: 0.4654\n",
      "Early stopping triggered after 15 epochs.\n",
      "Fold 4 Best_Spearman: 0.5196\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "[Epoch 0] Train Loss: 0.9629 | Val loss:0.9864 | Val Spearman: 0.3304\n",
      "New best Spearman: 0.3304 — Model saved.\n",
      "[Epoch 1] Train Loss: 0.9004 | Val loss:0.9713 | Val Spearman: 0.3867\n",
      "New best Spearman: 0.3867 — Model saved.\n",
      "[Epoch 2] Train Loss: 0.8284 | Val loss:0.9533 | Val Spearman: 0.3988\n",
      "New best Spearman: 0.3988 — Model saved.\n",
      "[Epoch 3] Train Loss: 0.7354 | Val loss:0.9365 | Val Spearman: 0.4678\n",
      "New best Spearman: 0.4678 — Model saved.\n",
      "[Epoch 4] Train Loss: 0.6769 | Val loss:0.9280 | Val Spearman: 0.5118\n",
      "New best Spearman: 0.5118 — Model saved.\n",
      "[Epoch 5] Train Loss: 0.6242 | Val loss:0.9438 | Val Spearman: 0.4770\n",
      "[Epoch 6] Train Loss: 0.6238 | Val loss:0.9384 | Val Spearman: 0.4958\n",
      "[Epoch 7] Train Loss: 0.6214 | Val loss:0.9396 | Val Spearman: 0.4817\n",
      "[Epoch 8] Train Loss: 0.6203 | Val loss:0.9402 | Val Spearman: 0.4639\n",
      "[Epoch 9] Train Loss: 0.6009 | Val loss:0.9353 | Val Spearman: 0.4901\n",
      "Early stopping triggered after 10 epochs.\n",
      "Fold 5 Best_Spearman: 0.5118\n",
      "\n",
      "===== Cross-Validation Complete =====\n",
      "Mean Spearman: 0.5359\n",
      "Std Spearman: 0.0453\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "\n",
    "\n",
    "save_dir = \"saved_models0.5-650M00\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "k=5\n",
    "gss = GroupShuffleSplit(n_splits=k, test_size=0.2, random_state=None)\n",
    "positions = df_train['pos']\n",
    "val_spearmans = []\n",
    "early_stop_patience = 5\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(gss.split(df_train, groups=positions)):\n",
    "    print(f\"\\n===== Fold {fold + 1}/{k} =====\")\n",
    "    df_train_cv = df_train.iloc[train_idx].reset_index(drop=True)\n",
    "    df_val_cv = df_train.iloc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = ProteinDataset(df_train_cv)\n",
    "    valid_dataset = ProteinDataset(df_val_cv)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        num_workers=config['num_workers'],\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        num_workers=config['num_workers'],\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "\n",
    "    model = ESMEffect(dropout_rate=0.5).to(device)\n",
    "    optimizer, scheduler = setup_optimizer_and_scheduler_esmeffect(model, train_loader, config['epochs'],\n",
    "    config['batch_size'], config['lr_esm'], config['lr_head'])\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_val_metric = -float('inf')\n",
    "    epochs_since_improvement = 0\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_metric = evaluate(model,valid_loader, criterion, device)\n",
    "        print(f\"[Epoch {epoch}] Train Loss: {train_loss:.4f} | Val loss:{val_metric['loss']:.4f} | Val Spearman: {val_metric['spearman']:.4f}\")\n",
    "\n",
    "        if val_metric['spearman'] > best_val_metric:\n",
    "            best_val_metric = val_metric['spearman']\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, f\"best_model_fold{fold + 1}.pt\"))\n",
    "            print(f\"New best Spearman: {best_val_metric:.4f} — Model saved.\")\n",
    "            epochs_since_improvement = 0\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "            \n",
    "        if epochs_since_improvement >= early_stop_patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "            break\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    val_spearmans.append(best_val_metric)\n",
    "    print(f\"Fold {fold + 1} Best_Spearman: {best_val_metric:.4f}\")\n",
    "    \n",
    "print(\"\\n===== Cross-Validation Complete =====\")\n",
    "print(f\"Mean Spearman: {np.mean(val_spearmans):.4f}\")\n",
    "print(f\"Std Spearman: {np.std(val_spearmans):.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from Fold 1...\n",
      "Loading model from Fold 2...\n",
      "Loading model from Fold 3...\n",
      "Loading model from Fold 4...\n",
      "Loading model from Fold 5...\n"
     ]
    }
   ],
   "source": [
    "# load test set predictions from all 5 folds\n",
    "all_preds = []\n",
    "for fold in range(1,6):\n",
    "    print(f\"Loading model from Fold {fold}...\")\n",
    "    model = ESMEffect(dropout_rate=0.5).to(device) \n",
    "    model.load_state_dict(torch.load(f\"{save_dir}/best_model_fold{fold}.pt\"))\n",
    "    preds = predict(model, df_test, config, device=device)\n",
    "    all_preds.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean of all 5 folds prediction\n",
    "all_preds = np.array(all_preds)\n",
    "mean_preds = np.mean(all_preds, axis=0) \n",
    "std_preds = np.std(all_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plot standard error of predictions\n",
    "plt.hist(std_preds, bins=30, edgecolor='black')\n",
    "plt.xlabel('Standard Predictions')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of std_preds')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['DMS_score_predicted'] = mean_preds\n",
    "df_test['std'] = std_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['mutant', 'DMS_score_predicted']].to_csv('ESM2_finetuend_predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 (Optional) Make new queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use uncertainty sampling for new queries\n",
    "top100_indices = np.argsort(std_preds)[-100:]\n",
    "mutants = [df_test.iloc[i]['mutant'] for i in topk_indices]\n",
    "\n",
    "# Write to a .txt file\n",
    "with open('query.txt', 'w') as f:\n",
    "    for mutant in mutants:\n",
    "        f.write(mutant+'\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python3.9(torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
